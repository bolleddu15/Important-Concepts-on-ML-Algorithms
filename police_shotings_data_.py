# -*- coding: utf-8 -*-
"""Police shotings data .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YkES4lA_PsK0Jni1Z09_9Ng8I0xKxPhM
"""

import pandas as pd
import matplotlib.pyplot as plt
df=pd.read_csv("/content/fatal-police-shootings-data.csv")
print(df)

# checking the missingn values
import pandas as pd
df = pd.read_csv('/content/fatal-police-shootings-data.csv')
missing_values = df.isnull().sum()
print(missing_values)

df_cleaned = df.dropna()
print(df_cleaned)

df['age'].fillna(df['age'].mean(), inplace=True)
print(df)

#categorical column of "race"
df['race'].fillna(df['race'].mode()[0], inplace=True)
print(df)
#categorical column of "flee"
df['flee_status'].fillna(df['flee_status'].mode()[0], inplace=True)
print(df)

missing_values = df.isnull().sum()
print(missing_values)

duplicate_rows = df[df.duplicated()]
print(duplicate_rows)

#checking the data formats
data_formats = df.dtypes
print(data_formats)

#data descriptive
summary_stats = df.describe()
print(summary_stats)

import pandas as pd

# Load your dataset
df = pd.read_csv('/content/fatal-police-shootings-data.csv')

# Choose the column you want to check for outliers
column_name = 'age'

# Calculate the IQR
Q1 = df[column_name].quantile(0.25)
Q3 = df[column_name].quantile(0.75)
IQR = Q3 - Q1

# Define the lower and upper bounds for outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Identify outliers
outliers = df[(df[column_name] < lower_bound) | (df[column_name] > upper_bound)]

# Print the rows with outliers
print(outliers)

# correlation Analysis
import seaborn as sns

correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True)
plt.title('Correlation Matrix')
plt.show()

# Example of a bar chart for a categorical column
df['race'].value_counts().plot(kind='bar')
plt.xlabel('Category')
plt.ylabel('Count')
plt.title('Categorical Column Distribution')
plt.show()

# Example of a bar chart for a categorical column
df['manner_of_death'].value_counts().plot(kind='bar')
plt.xlabel('Category')
plt.ylabel('Count')
plt.title('Categorical Column Distribution')
plt.show()

#Scatter Plot for Outliers
import matplotlib.pyplot as plt

# Create a scatter plot to visualize potential outliers
plt.figure(figsize=(8, 6))
plt.scatter(range(len(df['longitude'])), df['latitude'])
plt.title('Scatter Plot for Outliers')
plt.xlabel('Data Points')
plt.ylabel('Numeric Column')
plt.show()

#numeric column
import matplotlib.pyplot as plt
df['age'].hist()
plt.show()

#categorical column
df['age'].value_counts().plot(kind='bar')
plt.show()

df.head()

#check null values in the data
print(df.isna().any())

df.describe()

#To check Null values in dtype: bool
df = pd.DataFrame(df)
df.dropna()
print(df.isna().any())
print(df)

import pandas as pd
data = pd.read_csv('/content/fatal-police-shootings-data.csv')
columns = df.race
dataframe = df.data()
print(columns)

print(df.isna().any())

import pandas as pd
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt

# Create a DataFrame with the provided race and state data
data = pd.DataFrame({
    'Race': [
        'A', 'W', 'H', 'W', 'H', 'W', 'H', 'W', 'W', 'B', 'W', 'B', 'B', 'W', 'B',
        'W', 'W', 'B', 'W', 'W', 'H', 'W', 'H', 'H', 'W', 'W', 'B', 'W', 'B', 'W',
        'W', 'W', 'B', 'B', 'W', 'H', 'W', 'W', 'O', 'B', 'H', 'H', 'W', 'W', 'W',
        'B', 'W', 'H', 'B', 'W', 'B', 'W', 'B', 'H', 'W', 'W', 'B', 'W', 'H'
    ],
    'State': [
        'WA', 'OR', 'KS', 'CA', 'CO', 'OK', 'AZ', 'KS', 'IA', 'PA', 'CA', 'TX', 'OH',
        'IA', 'LA', 'MT', 'UT', 'AR', 'UT', 'AR', 'TX', 'IL', 'CA', 'NV', 'NM', 'TX',
        'MN', 'MO', 'VA', 'NJ', 'TX', 'UT', 'IN', 'CO', 'MN', 'CA', 'LA', 'TX', 'CA',
        'OK', 'CA', 'TX', 'AZ', 'KY', 'MA', 'AZ', 'CA', 'TX', 'MO', 'NH', 'TX', 'TX',
        'FL', 'TX', 'TX', 'ID', 'MD', 'TX', 'CO'
    ]
})

#DBSCAN clustering
# Encode categorical variables
label_encoder_race = LabelEncoder()
label_encoder_state = LabelEncoder()
data['Race'] = label_encoder_race.fit_transform(data['Race'])
data['State'] = label_encoder_state.fit_transform(data['State'])

# Prepare data for clustering
X = data[['Race', 'State']].values

# Perform DBSCAN clustering
dbscan = DBSCAN(eps=0.3, min_samples=5)  # Adjust the parameters as needed
data['Cluster'] = dbscan.fit_predict(X)

# Inverse transform the label encoding for visualization
data['Race'] = label_encoder_race.inverse_transform(data['Race'])
data['State'] = label_encoder_state.inverse_transform(data['State'])

# Visualize the clusters
unique_clusters = data['Cluster'].unique()
for i in unique_clusters:
    if i == -1:  # Noise points
        cluster_data = data[data['Cluster'] == i]
        plt.scatter(cluster_data['Race'], cluster_data['State'], label='Noise Points', marker='x')
    else:
        cluster_data = data[data['Cluster'] == i]
        plt.scatter(cluster_data['Race'], cluster_data['State'], label=f'Cluster {i}')

plt.xlabel('Race')
plt.ylabel('State')
plt.title('Clustering Based on Race and State (DBSCAN)')
plt.legend()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import AgglomerativeClustering
from sklearn.preprocessing import LabelEncoder
from scipy.cluster.hierarchy import dendrogram, linkage
import numpy as np

# Create a DataFrame with the provided race and state data
data = pd.DataFrame({
    'Race': [
        'A', 'W', 'H', 'W', 'H', 'W', 'H', 'W', 'W', 'B', 'W', 'B', 'B', 'W', 'B',
        'W', 'W', 'B', 'W', 'W', 'H', 'W', 'H', 'H', 'W', 'W', 'B', 'W', 'B', 'W',
        'W', 'W', 'B', 'B', 'W', 'H', 'W', 'W', 'O', 'B', 'H', 'H', 'W', 'W', 'W',
        'B', 'W', 'H', 'B', 'W', 'B', 'W', 'B', 'H', 'W', 'W', 'B', 'W', 'H'
    ],
    'State': [
        'WA', 'OR', 'KS', 'CA', 'CO', 'OK', 'AZ', 'KS', 'IA', 'PA', 'CA', 'TX', 'OH',
        'IA', 'LA', 'MT', 'UT', 'AR', 'UT', 'AR', 'TX', 'IL', 'CA', 'NV', 'NM', 'TX',
        'MN', 'MO', 'VA', 'NJ', 'TX', 'UT', 'IN', 'CO', 'MN', 'CA', 'LA', 'TX', 'CA',
        'OK', 'CA', 'TX', 'AZ', 'KY', 'MA', 'AZ', 'CA', 'TX', 'MO', 'NH', 'TX', 'TX',
        'FL', 'TX', 'TX', 'ID', 'MD', 'TX', 'CO'
    ]
})

# Kmeans clustering
# Encode categorical variables
label_encoder_race = LabelEncoder()
label_encoder_state = LabelEncoder()
data['Race'] = label_encoder_race.fit_transform(data['Race'])
data['State'] = label_encoder_state.fit_transform(data['State'])

# Prepare data for hierarchical clustering
X = data[['Race', 'State']].values

# Perform hierarchical clustering
linkage_matrix = linkage(X, method='ward')
n_clusters = 3  # Adjust the number of clusters as needed
hierarchical = AgglomerativeClustering(n_clusters=n_clusters, affinity='euclidean', linkage='ward')
data['Cluster'] = hierarchical.fit_predict(X)

# Inverse transform the label encoding for visualization
data['Race'] = label_encoder_race.inverse_transform(data['Race'])
data['State'] = label_encoder_state.inverse_transform(data['State'])

# Visualize the clusters using a dendrogram (optional)
plt.figure(figsize=(12, 6))
dendrogram(linkage_matrix)
plt.title('Dendrogram for Hierarchical Clustering')
plt.show()

# Visualize the clusters
for i in range(n_clusters):
    cluster_data = data[data['Cluster'] == i]
    plt.scatter(cluster_data['Race'], cluster_data['State'], label=f'Cluster {i + 1}')

plt.xlabel('Race')
plt.ylabel('State')
plt.title('Clustering Based on Race and State (Hierarchical Clustering)')
plt.legend()
plt.show()

from sklearn.cluster import DBSCAN

# Create DBSCAN instance
# Adjust eps and min_samples as needed
dbscan = DBSCAN(eps=3, min_samples=2)

# Fit the DBSCAN model
data['Cluster'] = dbscan.fit_predict(X)

# Visualize the clusters including outliers
unique_clusters = set(data['Cluster'])
for cluster in unique_clusters:
    cluster_data = data[data['Cluster'] == cluster]
    if cluster == -1:
        # Outliers can be plotted with a different marker or color
        plt.scatter(cluster_data['Race'], cluster_data['State'], label='Outliers', marker='x')
    else:
        plt.scatter(cluster_data['Race'], cluster_data['State'], label=f'Cluster {cluster + 1}')

plt.xlabel('Race')
plt.ylabel('State')
plt.title('DBSCAN Clustering Based on Race and State')
plt.legend()
plt.show()

import pandas as pd
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt

# Load your data from an Excel file
data = pd.read_csv('/content/fatal-police-shootings-data.csv')  # Replace with your file path

# Assume 'Race' and 'State' columns exist in your Excel file
# If they have different names, replace 'Race' and 'State' with the actual column names

# Encode categorical variables
label_encoder_race = LabelEncoder()
label_encoder_state = LabelEncoder()
data['race_encoded'] = label_encoder_race.fit_transform(data['race'])
data['state_encoded'] = label_encoder_state.fit_transform(data['state'])

# Prepare data for clustering
X = data[['race_encoded', 'state_encoded']].values

# Perform DBSCAN clustering
dbscan = DBSCAN(eps=0.5, min_samples=5)  # Adjust eps and min_samples as needed
data['Cluster'] = dbscan.fit_predict(X)

# Plot the clusters and outliers
unique_clusters = data['Cluster'].unique()
for cluster in unique_clusters:
    if cluster == -1:  # Cluster -1 represents outliers
        cluster_data = data[data['Cluster'] == cluster]
        plt.scatter(
            cluster_data['race_encoded'], cluster_data['state_encoded'], label=f'Outliers',
            color='red', marker='x', s=100  # Change color to red and use a large 'x' marker
        )
    else:
        cluster_data = data[data['Cluster'] == cluster]
        plt.scatter(cluster_data['race_encoded'], cluster_data['state_encoded'], label=f'Cluster {cluster}')

plt.xlabel('Race')
plt.ylabel('State')
plt.title('DBSCAN Clustering of Race and State')
plt.legend()
plt.show()



!pip install cartopy
import cartopy

import folium
import cartopy.crs as ccrs
# Create a base map
locations = list(zip(df['latitude'], df['longitude']))
lats = [lat for lat, _ in locations]
lons = [lon for _, lon in locations]

fig, ax = plt.subplots(subplot_kw={'projection': ccrs.PlateCarree()}, figsize=(10, 6))
ax.scatter(lons, lats, c='blue', marker='o')
ax.coastlines()
ax.set_xlabel('Longitude')
ax.set_ylabel('Latitude')
ax.set_title('Geographic Locations')
plt.show()

from numpy import median
from numpy.core.fromnumeric import mean
##Age distribution for people killed by police

allAges=df["age"]
# Descriptive Statistics for AGE DATA.
max_Age=allAges.max()
min_Age=allAges.min()
mean_allAges = allAges.mean()
median_allAges = allAges.median()
std_dev_allAges=allAges.std()
kurtosis_allAges=allAges.kurtosis()
print("Maximum Age",max_Age)
print("Minimum Age",min_Age)
print("mean of Age",mean_allAges)
print("standard deviation of Age",std_dev_allAges)
print("Median Age",median_allAges)
print("kurtosis of Age",kurtosis_allAges)
print("skewness of Age",allAges.skew())

#Plotting Histogram
plt.hist(allAges)
plt.title("HISTOGRAM FOR AGE")
plt.show()

agedistribution=df[['age','gender','race']]

# Using a boolean mask to filter the DataFrame for black people's ages
black_ages = agedistribution[agedistribution['race'] == 'B']['age']

# Using a boolean mask to filter the DataFrame for white people's ages
white_ages = agedistribution[agedistribution['race'] == 'W']['age']

print("Ages of black people:", black_ages)
print("Ages of white people:", white_ages)

print("Descriptive statistics of the distribution of ages for black people")
max_Bage=black_ages.max()
min_Bage=black_ages.min()
mean_Bage = black_ages.mean()
median_Bage = black_ages.median()
std_dev_Bage=black_ages.std()
kurtosis_Bage=black_ages.kurtosis()
print("Maximum Age",max_Bage)
print("Minimum Age",min_Bage)
print("mean of Age",mean_Bage)
print("standard deviation of Age",std_dev_Bage)
print("Median Age",median_Bage)
print("kurtosis of Age",kurtosis_Bage)
print("skewness of Age",black_ages.skew())
print("\n")
print("Descriptive statistics of the distribution of ages for white people")
max_Wage=white_ages.max()
min_Wage=white_ages.min()
mean_Wage = white_ages.mean()
median_Wage = white_ages.median()
std_dev_Wage=white_ages.std()
kurtosis_Wage=white_ages.kurtosis()
print("Maximum Age",max_Wage)
print("Minimum Age",min_Wage)
print("mean of Age",mean_Wage)
print("standard deviation of Age",std_dev_Wage)
print("Median Age",median_Wage)
print("kurtosis of Age",kurtosis_Wage)
print("skewness of Age",white_ages.skew())


#Plotting Histogram
plt.hist(black_ages, edgecolor='black',alpha=0.5, label='black people Age',bins=14)
plt.hist(white_ages, edgecolor='red',alpha=0.5, label='white people Age',bins=14)
plt.xlabel("Age")
plt.ylabel("Number of People")
plt.legend(loc='upper right')
plt.show()

##Mean Age Per Race
mean_age_per_race=agedistribution.groupby('race').mean()
# Load the data
Race = ['ASIAN', 'BLACK', 'HISPANIC', 'AMERICAN', 'OTHER', 'WHITE']
Mean_Age = [35.862069, 32.998991, 33.749038, 32.594595, 33.909091, 40.196643]

# Create the bar chart
plt.bar(Race, Mean_Age, color=['green', 'black','blue','orange','red','white'],edgecolor=['green', 'black','blue','orange','red','black'])

# Add labels and title to the chart
plt.xlabel('Race')
plt.ylabel('Mean_Age')
plt.title('Mean Age per Race')

# Display the chart
plt.tight_layout()
plt.show()

##Implement logistic regression to predict people shot were mentally-ill based on race,threat_type,age and gender
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Selecting the specified columns
X = df[['race', 'threat_type','age','gender']]
y = df['was_mental_illness_related']

# Preprocessing: One-hot encoding for categorical variables
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), ['race', 'threat_type','age','gender'])
    ])

# Creating a pipeline that first transforms the data and then applies logistic regression
pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                           ('classifier', LogisticRegression())])

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fitting the model
pipeline.fit(X_train, y_train)

# Predictions and Evaluation
predictions = pipeline.predict(X_test)
print(confusion_matrix(y_test, predictions))
print(classification_report(y_test, predictions))

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve

# Confusion Matrix
cm = confusion_matrix(y_test, predictions)
sns.heatmap(cm, annot=True, fmt="d", cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

'''
# ROC Curve
fpr, tpr, _ = roc_curve(y_test, pipeline.predict_proba(X_test)[:, 1])
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

# Precision-Recall Curve
precision, recall, _ = precision_recall_curve(y_test, pipeline.predict_proba(X_test)[:, 1])

plt.figure()
plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="lower left")
plt.show()
'''